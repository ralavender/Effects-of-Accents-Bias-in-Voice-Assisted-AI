{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqvEa2LTHwyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee940af-008c-4b03-960f-63ba16b427c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 37 (delta 5), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (37/37), 979.94 KiB | 7.60 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Nexdata-AI/593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone.git ## get dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone ## make sure it exists"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ6DeMXYKvyO",
        "outputId": "c19822ec-426b-48b9-803c-fb361debee85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rX_kHcEK0Bq",
        "outputId": "cbda40fc-efc0-4757-fee6-a5eb7c79a5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md\t     T0055G0007S0001.wav  T0055G0030S0021.wav  T0055G0064S0018.wav\n",
            "T0055G0002S0001.txt  T0055G0009S0148.txt  T0055G0032S0125.txt  T0055G0066S0004.txt\n",
            "T0055G0002S0001.wav  T0055G0009S0148.wav  T0055G0032S0125.wav  T0055G0066S0004.wav\n",
            "T0055G0003S0009.txt  T0055G0023S0004.txt  T0055G0033S0001.txt\n",
            "T0055G0003S0009.wav  T0055G0023S0004.wav  T0055G0033S0001.wav\n",
            "T0055G0007S0001.txt  T0055G0030S0021.txt  T0055G0064S0018.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install ffmpeg\n",
        "!apt-get install ffmpeg\n",
        "## install whisper AI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-zDqm6SK2Qn",
        "outputId": "b6a2a15c-59d5-4e94-dd6b-d1dee8bb46f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-5tuz5o1s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-5tuz5o1s\n",
            "  Resolved https://github.com/openai/whisper.git to commit fc5ded7d9045c693692f13853857c3f8baea3a7b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803579 sha256=39a59fb8d6960e3f9a4c43f051f2e80ede0df43fcd1d948ebc4fadf9ebf310a0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tajyz11s/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=886954c0524354081eb66bc91366d70d973717e2708278f29ca4ea1af9587b3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os"
      ],
      "metadata": {
        "id": "g3gZk5QpLf7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSn_n1jnL01N",
        "outputId": "4c378039-dc77-4df4-d9b6-36ac5c8c10b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder containing .wav files\n",
        "wav_folder = '/content/593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone'\n",
        "\n",
        "# List all .wav files in the folder\n",
        "wav_files = [f for f in os.listdir(wav_folder) if f.endswith('.wav')]\n",
        "\n",
        "# Loop through and transcribe each audio file\n",
        "for wav_file in wav_files:\n",
        "    file_path = os.path.join(wav_folder, wav_file)\n",
        "    print(f\"Transcribing {wav_file}...\")\n",
        "\n",
        "    # Perform transcription\n",
        "    result = model.transcribe(file_path)\n",
        "\n",
        "    # Print transcription\n",
        "    print(f\"Transcription for {wav_file}:\")\n",
        "    print(result['text'])\n",
        "    print(\"=\" * 80)  # Separator for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMVLsk_CMHOQ",
        "outputId": "e14dad25-9461-4a97-c6e6-0e302c642cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing T0055G0032S0125.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0032S0125.wav:\n",
            " Hey, hold out a cave in the slowfield for light.\n",
            "================================================================================\n",
            "Transcribing T0055G0009S0148.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0009S0148.wav:\n",
            " 끝에 beat punished 그래서 너무 두abilities\n",
            "================================================================================\n",
            "Transcribing T0055G0003S0009.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0003S0009.wav:\n",
            " Her tricks had fallen in, making her look old.\n",
            "================================================================================\n",
            "Transcribing T0055G0007S0001.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0007S0001.wav:\n",
            " No one could know why he did like that.\n",
            "================================================================================\n",
            "Transcribing T0055G0064S0018.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0064S0018.wav:\n",
            " Attack from above.\n",
            "================================================================================\n",
            "Transcribing T0055G0030S0021.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0030S0021.wav:\n",
            " No me, okay, I'm in sleep.\n",
            "================================================================================\n",
            "Transcribing T0055G0023S0004.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0023S0004.wav:\n",
            " The box is cute of the tree.\n",
            "================================================================================\n",
            "Transcribing T0055G0002S0001.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0002S0001.wav:\n",
            " 오늘도\n",
            "================================================================================\n",
            "Transcribing T0055G0066S0004.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0066S0004.wav:\n",
            " He asked them to discontinue flights over the island.\n",
            "================================================================================\n",
            "Transcribing T0055G0033S0001.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for T0055G0033S0001.wav:\n",
            " I can lend this book to you. I had it date two weeks ago.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7T61fpVNNEL",
        "outputId": "701d2f9a-4c48-425c-90f2-5982af63004b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.5 rapidfuzz-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from jiwer import wer, cer\n",
        "\n",
        "# Path to dataset\n",
        "wav_folder = '/content/593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone'\n",
        "\n",
        "# Updated Transcriptions generated by Whisper\n",
        "transcriptions = {\n",
        "    \"T0055G0032S0125.wav\": \"Hey, hold out a cave in the slowfield for light.\",\n",
        "    \"T0055G0009S0148.wav\": \"끝에 beat punished 그래서 너무 두abilities\",\n",
        "    \"T0055G0003S0009.wav\": \"Her tricks had fallen in, making her look old.\",\n",
        "    \"T0055G0007S0001.wav:\": \"No one could know why he did like that.\",\n",
        "    \"T0055G0064S0018.wav\": \"Attack from above.\",\n",
        "    \"T0055G0030S0021.wav\": \"No me, okay, I'm in sleep.\",\n",
        "    \"T0055G0023S0004.wav\": \"The box is cute of the tree.\",\n",
        "    \"T0055G0002S0001.wav:\": \"오늘도\",\n",
        "    \"T0055G0066S0004.wav\": \"He asked them to discontinue flights over the island.\",\n",
        "    \"T0055G0033S0001.wav\": \"I can lend this book to you. I had it date two weeks ago.\"\n",
        "}\n",
        "\n",
        "# Load ground truth text files\n",
        "ground_truth = {}\n",
        "for txt_file in os.listdir(wav_folder):\n",
        "    if txt_file.endswith('.txt'):\n",
        "        with open(os.path.join(wav_folder, txt_file), 'r', encoding='utf-8') as f:\n",
        "            ground_truth[txt_file.replace('.txt', '.wav')] = f.read().strip()\n",
        "\n",
        "# Debugging: Check loaded files\n",
        "print(\"Ground truth files loaded:\", ground_truth)\n",
        "print(\"Transcriptions keys:\", transcriptions.keys())\n",
        "print(\"Ground truth keys:\", ground_truth.keys())\n",
        "\n",
        "output_file = \"wer_cer_results.txt\"\n",
        "\n",
        "# Open a file to save the results\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for filename, generated_text in transcriptions.items():\n",
        "        if filename in ground_truth:\n",
        "            reference_text = ground_truth[filename]\n",
        "            error_rate_wer = wer(reference_text, generated_text)\n",
        "            error_rate_cer = cer(reference_text, generated_text)\n",
        "\n",
        "            # Write results to the file\n",
        "            f.write(f\"WER for {filename}: {error_rate_wer:.2%}\\n\")\n",
        "            f.write(f\"CER for {filename}: {error_rate_cer:.2%}\\n\")\n",
        "            f.write(f\"Reference: {reference_text}\\n\")\n",
        "            f.write(f\"Transcription: {generated_text}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "            # Print results for verification\n",
        "            print(f\"WER for {filename}: {error_rate_wer:.2%}\")\n",
        "            print(f\"CER for {filename}: {error_rate_cer:.2%}\")\n",
        "            print(f\"Reference: {reference_text}\")\n",
        "            print(f\"Transcription: {generated_text}\")\n",
        "            print(\"=\" * 80)\n",
        "        else:\n",
        "            f.write(f\"Ground truth not found for {filename}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            print(f\"Ground truth not found for {filename}\")\n",
        "\n",
        "print(f\"All WER and CER results saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvcQTcu9So4B",
        "outputId": "534a802d-d39e-4166-fc7d-2349e293bcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth files loaded: {'T0055G0032S0125.wav': 'He hollowed out a cave in the snowfield for night.', 'wer_results.wav': \"WER for T0055G0032S0125.wav: 40.00%\\nReference: He hollowed out a cave in the snowfield for night.\\nTranscription: Hey, hold out a cave in the slowfield for light.\\n================================================================================\\nWER for T0055G0009S0148.wav: 100.00%\\nReference: Finally, the biggest problem of all: the toilet.\\nTranscription: 끝에 beat punished 그래서 너무 두abilities\\n================================================================================\\nWER for T0055G0003S0009.wav: 37.50%\\nReference: Her cheeks had fallen in,making her look old.\\nTranscription: Her tricks had fallen in, making her look old.\\n================================================================================\\nGround truth not found for T0055G0007S0001.wav:\\n================================================================================\\nWER for T0055G0064S0018.wav: 33.33%\\nReference: Attack from above!\\nTranscription: Attack from above.\\n================================================================================\\nWER for T0055G0030S0021.wav: 100.00%\\nReference: No milk. I'm slimming.\\nTranscription: No me, okay, I'm in sleep.\\n================================================================================\\nWER for T0055G0023S0004.wav: 66.67%\\nReference: The bark scaled off the tree.\\nTranscription: The box is cute of the tree.\\n================================================================================\\nGround truth not found for T0055G0002S0001.wav:\\n================================================================================\\nWER for T0055G0066S0004.wav: 0.00%\\nReference: He asked them to discontinue flights over the island.\\nTranscription: He asked them to discontinue flights over the island.\\n================================================================================\\nWER for T0055G0033S0001.wav: 35.71%\\nReference: I can lend this book to you !I have finished it two weeks ago.\\nTranscription: I can lend this book to you. I had it date two weeks ago.\\n================================================================================\", 'T0055G0023S0004.wav': 'The bark scaled off the tree.', 'T0055G0003S0009.wav': 'Her cheeks had fallen in,making her look old.', 'T0055G0002S0001.wav': \"We're focused on small things: Do I have my pierce?\", 'T0055G0066S0004.wav': 'He asked them to discontinue flights over the island.', 'wer_cer_results.wav': \"WER for T0055G0032S0125.wav: 40.00%\\nCER for T0055G0032S0125.wav: 16.00%\\nReference: He hollowed out a cave in the snowfield for night.\\nTranscription: Hey, hold out a cave in the slowfield for light.\\n================================================================================\\nWER for T0055G0009S0148.wav: 100.00%\\nCER for T0055G0009S0148.wav: 79.17%\\nReference: Finally, the biggest problem of all: the toilet.\\nTranscription: 끝에 beat punished 그래서 너무 두abilities\\n================================================================================\\nWER for T0055G0003S0009.wav: 37.50%\\nCER for T0055G0003S0009.wav: 11.11%\\nReference: Her cheeks had fallen in,making her look old.\\nTranscription: Her tricks had fallen in, making her look old.\\n================================================================================\\nGround truth not found for T0055G0007S0001.wav:\\n================================================================================\\nWER for T0055G0064S0018.wav: 33.33%\\nCER for T0055G0064S0018.wav: 5.56%\\nReference: Attack from above!\\nTranscription: Attack from above.\\n================================================================================\\nWER for T0055G0030S0021.wav: 100.00%\\nCER for T0055G0030S0021.wav: 68.18%\\nReference: No milk. I'm slimming.\\nTranscription: No me, okay, I'm in sleep.\\n================================================================================\\nWER for T0055G0023S0004.wav: 66.67%\\nCER for T0055G0023S0004.wav: 31.03%\\nReference: The bark scaled off the tree.\\nTranscription: The box is cute of the tree.\\n================================================================================\\nGround truth not found for T0055G0002S0001.wav:\\n================================================================================\\nWER for T0055G0066S0004.wav: 0.00%\\nCER for T0055G0066S0004.wav: 0.00%\\nReference: He asked them to discontinue flights over the island.\\nTranscription: He asked them to discontinue flights over the island.\\n================================================================================\\nWER for T0055G0033S0001.wav: 35.71%\\nCER for T0055G0033S0001.wav: 20.97%\\nReference: I can lend this book to you !I have finished it two weeks ago.\\nTranscription: I can lend this book to you. I had it date two weeks ago.\\n================================================================================\", 'T0055G0064S0018.wav': 'Attack from above!', 'T0055G0033S0001.wav': 'I can lend this book to you !I have finished it two weeks ago.', 'T0055G0007S0001.wav': 'No one could know why he did like that.', 'T0055G0009S0148.wav': 'Finally, the biggest problem of all: the toilet.', 'T0055G0030S0021.wav': \"No milk. I'm slimming.\"}\n",
            "Transcriptions keys: dict_keys(['T0055G0032S0125.wav', 'T0055G0009S0148.wav', 'T0055G0003S0009.wav', 'T0055G0007S0001.wav:', 'T0055G0064S0018.wav', 'T0055G0030S0021.wav', 'T0055G0023S0004.wav', 'T0055G0002S0001.wav:', 'T0055G0066S0004.wav', 'T0055G0033S0001.wav'])\n",
            "Ground truth keys: dict_keys(['T0055G0032S0125.wav', 'wer_results.wav', 'T0055G0023S0004.wav', 'T0055G0003S0009.wav', 'T0055G0002S0001.wav', 'T0055G0066S0004.wav', 'wer_cer_results.wav', 'T0055G0064S0018.wav', 'T0055G0033S0001.wav', 'T0055G0007S0001.wav', 'T0055G0009S0148.wav', 'T0055G0030S0021.wav'])\n",
            "WER for T0055G0032S0125.wav: 40.00%\n",
            "CER for T0055G0032S0125.wav: 16.00%\n",
            "Reference: He hollowed out a cave in the snowfield for night.\n",
            "Transcription: Hey, hold out a cave in the slowfield for light.\n",
            "================================================================================\n",
            "WER for T0055G0009S0148.wav: 100.00%\n",
            "CER for T0055G0009S0148.wav: 79.17%\n",
            "Reference: Finally, the biggest problem of all: the toilet.\n",
            "Transcription: 끝에 beat punished 그래서 너무 두abilities\n",
            "================================================================================\n",
            "WER for T0055G0003S0009.wav: 37.50%\n",
            "CER for T0055G0003S0009.wav: 11.11%\n",
            "Reference: Her cheeks had fallen in,making her look old.\n",
            "Transcription: Her tricks had fallen in, making her look old.\n",
            "================================================================================\n",
            "Ground truth not found for T0055G0007S0001.wav:\n",
            "WER for T0055G0064S0018.wav: 33.33%\n",
            "CER for T0055G0064S0018.wav: 5.56%\n",
            "Reference: Attack from above!\n",
            "Transcription: Attack from above.\n",
            "================================================================================\n",
            "WER for T0055G0030S0021.wav: 100.00%\n",
            "CER for T0055G0030S0021.wav: 68.18%\n",
            "Reference: No milk. I'm slimming.\n",
            "Transcription: No me, okay, I'm in sleep.\n",
            "================================================================================\n",
            "WER for T0055G0023S0004.wav: 66.67%\n",
            "CER for T0055G0023S0004.wav: 31.03%\n",
            "Reference: The bark scaled off the tree.\n",
            "Transcription: The box is cute of the tree.\n",
            "================================================================================\n",
            "Ground truth not found for T0055G0002S0001.wav:\n",
            "WER for T0055G0066S0004.wav: 0.00%\n",
            "CER for T0055G0066S0004.wav: 0.00%\n",
            "Reference: He asked them to discontinue flights over the island.\n",
            "Transcription: He asked them to discontinue flights over the island.\n",
            "================================================================================\n",
            "WER for T0055G0033S0001.wav: 35.71%\n",
            "CER for T0055G0033S0001.wav: 20.97%\n",
            "Reference: I can lend this book to you !I have finished it two weeks ago.\n",
            "Transcription: I can lend this book to you. I had it date two weeks ago.\n",
            "================================================================================\n",
            "All WER and CER results saved to wer_cer_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"wer_results.txt\"\n",
        "\n",
        "# Open a file to save the results\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for filename, generated_text in transcriptions.items():\n",
        "        if filename in ground_truth:\n",
        "            reference_text = ground_truth[filename]\n",
        "            error_rate = wer(reference_text, generated_text)\n",
        "\n",
        "            # Write results to the file\n",
        "            f.write(f\"WER for {filename}: {error_rate:.2%}\\n\")\n",
        "            f.write(f\"Reference: {reference_text}\\n\")\n",
        "            f.write(f\"Transcription: {generated_text}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "        else:\n",
        "            f.write(f\"Ground truth not found for {filename}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"All WER results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUG7wo6ZOHiz",
        "outputId": "52930ef8-0ecc-4fb7-97bd-87b041130805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All WER results saved to wer_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from jiwer import wer, cer\n",
        "\n",
        "# Path to your dataset\n",
        "wav_folder = '/content/593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone'\n",
        "\n",
        "# Updated Transcriptions generated by Whisper\n",
        "transcriptions = {\n",
        "    \"T0055G0032S0125.wav\": \"Hey, hold out a cave in the slowfield for light.\",\n",
        "    \"T0055G0009S0148.wav\": \"끝에 beat punished 그래서 너무 두abilities\",\n",
        "    \"T0055G0003S0009.wav\": \"Her tricks had fallen in, making her look old.\",\n",
        "    \"T0055G0007S0001.wav:\": \"No one could know why he did like that.\",\n",
        "    \"T0055G0064S0018.wav\": \"Attack from above.\",\n",
        "    \"T0055G0030S0021.wav\": \"No me, okay, I'm in sleep.\",\n",
        "    \"T0055G0023S0004.wav\": \"The box is cute of the tree.\",\n",
        "    \"T0055G0002S0001.wav:\": \"오늘도\",\n",
        "    \"T0055G0066S0004.wav\": \"He asked them to discontinue flights over the island.\",\n",
        "    \"T0055G0033S0001.wav\": \"I can lend this book to you. I had it date two weeks ago.\"\n",
        "}\n",
        "\n",
        "# Load ground truth text files\n",
        "ground_truth = {}\n",
        "for txt_file in os.listdir(wav_folder):\n",
        "    if txt_file.endswith('.txt'):\n",
        "        with open(os.path.join(wav_folder, txt_file), 'r', encoding='utf-8') as f:\n",
        "            ground_truth[txt_file.replace('.txt', '.wav')] = f.read().strip()\n",
        "\n",
        "# Variables to calculate total WER and CER\n",
        "total_words = 0\n",
        "total_chars = 0\n",
        "total_word_errors = 0\n",
        "total_char_errors = 0\n",
        "\n",
        "output_file = \"wer_cer_results_with_totals.txt\"\n",
        "\n",
        "# Open a file to save the results\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for filename, generated_text in transcriptions.items():\n",
        "        if filename in ground_truth:\n",
        "            reference_text = ground_truth[filename]\n",
        "\n",
        "            # Calculate WER and CER\n",
        "            word_error_rate = wer(reference_text, generated_text)\n",
        "            char_error_rate = cer(reference_text, generated_text)\n",
        "\n",
        "            # Calculate individual word and char counts\n",
        "            word_count = len(reference_text.split())\n",
        "            char_count = len(reference_text.replace(\" \", \"\"))  # Exclude spaces\n",
        "\n",
        "            # Aggregate totals\n",
        "            total_words += word_count\n",
        "            total_chars += char_count\n",
        "            total_word_errors += word_error_rate * word_count\n",
        "            total_char_errors += char_error_rate * char_count\n",
        "\n",
        "            # Write individual results to the file\n",
        "            f.write(f\"WER for {filename}: {word_error_rate:.2%}\\n\")\n",
        "            f.write(f\"CER for {filename}: {char_error_rate:.2%}\\n\")\n",
        "            f.write(f\"Reference: {reference_text}\\n\")\n",
        "            f.write(f\"Transcription: {generated_text}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "            # Print individual results for debugging\n",
        "            print(f\"WER for {filename}: {word_error_rate:.2%}\")\n",
        "            print(f\"CER for {filename}: {char_error_rate:.2%}\")\n",
        "        else:\n",
        "            f.write(f\"Ground truth not found for {filename}\\n\")\n",
        "            print(f\"Ground truth not found for {filename}\")\n",
        "\n",
        "    # Calculate overall WER and CER\n",
        "    total_wer = total_word_errors / total_words if total_words > 0 else 0\n",
        "    total_cer = total_char_errors / total_chars if total_chars > 0 else 0\n",
        "\n",
        "    # Write total results to the file\n",
        "    f.write(\"\\nOverall Results:\\n\")\n",
        "    f.write(f\"Total WER: {total_wer:.2%}\\n\")\n",
        "    f.write(f\"Total CER: {total_cer:.2%}\\n\")\n",
        "    print(\"\\nOverall Results:\")\n",
        "    print(f\"Total WER: {total_wer:.2%}\")\n",
        "    print(f\"Total CER: {total_cer:.2%}\")\n",
        "\n",
        "print(f\"All WER and CER results, including totals, saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP8_gfeKTGwC",
        "outputId": "f0710464-4738-4526-b9b4-7fa244d4fdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER for T0055G0032S0125.wav: 40.00%\n",
            "CER for T0055G0032S0125.wav: 16.00%\n",
            "WER for T0055G0009S0148.wav: 100.00%\n",
            "CER for T0055G0009S0148.wav: 79.17%\n",
            "WER for T0055G0003S0009.wav: 37.50%\n",
            "CER for T0055G0003S0009.wav: 11.11%\n",
            "Ground truth not found for T0055G0007S0001.wav:\n",
            "WER for T0055G0064S0018.wav: 33.33%\n",
            "CER for T0055G0064S0018.wav: 5.56%\n",
            "WER for T0055G0030S0021.wav: 100.00%\n",
            "CER for T0055G0030S0021.wav: 68.18%\n",
            "WER for T0055G0023S0004.wav: 66.67%\n",
            "CER for T0055G0023S0004.wav: 31.03%\n",
            "Ground truth not found for T0055G0002S0001.wav:\n",
            "WER for T0055G0066S0004.wav: 0.00%\n",
            "CER for T0055G0066S0004.wav: 0.00%\n",
            "WER for T0055G0033S0001.wav: 35.71%\n",
            "CER for T0055G0033S0001.wav: 20.97%\n",
            "\n",
            "Overall Results:\n",
            "Total WER: 46.77%\n",
            "Total CER: 27.40%\n",
            "All WER and CER results, including totals, saved to wer_cer_results_with_totals.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Results: Tested WER and CER of Whisper AI for each Chinese-speaking-English audio samples as well as the overall WER and CER. NOTE: The WER and CER could not be calculated for 2 of the samples because Whisper AI transcribed the audio file as a completely different language(Korean) so WER and CER could not be applied because we are testing for WER and CER English accuracy."
      ],
      "metadata": {
        "id": "kuLIRehMT9AM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing WER and CER between female and male samples to address intersectionality."
      ],
      "metadata": {
        "id": "PB8xuKqaXWfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_metadata = {\n",
        "    \"T0055G0032S0125.wav\": \"female\",\n",
        "    \"T0055G0009S0148.wav\": \"male\",\n",
        "    \"T0055G0003S0009.wav\": \"female\",\n",
        "    \"T0055G0007S0001.wav\": \"male\",\n",
        "    \"T0055G0064S0018.wav\": \"male\",\n",
        "    \"T0055G0030S0021.wav\": \"female\",\n",
        "    \"T0055G0023S0004.wav\": \"male\",\n",
        "    \"T0055G0002S0001.wav\": \"female\",\n",
        "    \"T0055G0066S0004.wav\": \"male\",\n",
        "    \"T0055G0033S0001.wav\": \"female\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "v22meC5-Xcv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from jiwer import wer, cer\n",
        "\n",
        "# Path to your dataset\n",
        "wav_folder = '/content/593-Hours-Chinese-Speaking-English-Speech-Data-by-Mobile-phone'\n",
        "\n",
        "# Updated Transcriptions generated by Whisper\n",
        "transcriptions = {\n",
        "    \"T0055G0032S0125.wav\": \"Hey, hold out a cave in the slowfield for light.\",\n",
        "    \"T0055G0009S0148.wav\": \"끝에 beat punished 그래서 너무 두abilities\",\n",
        "    \"T0055G0003S0009.wav\": \"Her tricks had fallen in, making her look old.\",\n",
        "    \"T0055G0007S0001.wav\": \"No one could know why he did like that.\",\n",
        "    \"T0055G0064S0018.wav\": \"Attack from above.\",\n",
        "    \"T0055G0030S0021.wav\": \"No me, okay, I'm in sleep.\",\n",
        "    \"T0055G0023S0004.wav\": \"The box is cute of the tree.\",\n",
        "    \"T0055G0002S0001.wav\": \"오늘도\",\n",
        "    \"T0055G0066S0004.wav\": \"He asked them to discontinue flights over the island.\",\n",
        "    \"T0055G0033S0001.wav\": \"I can lend this book to you. I had it date two weeks ago.\"\n",
        "}\n",
        "\n",
        "# Gender metadata\n",
        "gender_metadata = {\n",
        "    \"T0055G0032S0125.wav\": \"female\",\n",
        "    \"T0055G0009S0148.wav\": \"male\",\n",
        "    \"T0055G0003S0009.wav\": \"female\",\n",
        "    \"T0055G0007S0001.wav\": \"male\",\n",
        "    \"T0055G0064S0018.wav\": \"male\",\n",
        "    \"T0055G0030S0021.wav\": \"female\",\n",
        "    \"T0055G0023S0004.wav\": \"male\",\n",
        "    \"T0055G0002S0001.wav\": \"female\",\n",
        "    \"T0055G0066S0004.wav\": \"male\",\n",
        "    \"T0055G0033S0001.wav\": \"female\"\n",
        "}\n",
        "\n",
        "# Load ground truth text files\n",
        "ground_truth = {}\n",
        "for txt_file in os.listdir(wav_folder):\n",
        "    if txt_file.endswith('.txt'):\n",
        "        with open(os.path.join(wav_folder, txt_file), 'r', encoding='utf-8') as f:\n",
        "            ground_truth[txt_file.replace('.txt', '.wav')] = f.read().strip()\n",
        "\n",
        "# Variables for male and female WER/CER\n",
        "male_total_words = female_total_words = 0\n",
        "male_total_chars = female_total_chars = 0\n",
        "male_word_errors = female_word_errors = 0\n",
        "male_char_errors = female_char_errors = 0\n",
        "\n",
        "# Calculate WER and CER by gender\n",
        "for filename, generated_text in transcriptions.items():\n",
        "    if filename in ground_truth and filename in gender_metadata:\n",
        "        reference_text = ground_truth[filename]\n",
        "        word_error_rate = wer(reference_text, generated_text)\n",
        "        char_error_rate = cer(reference_text, generated_text)\n",
        "\n",
        "        word_count = len(reference_text.split())\n",
        "        char_count = len(reference_text.replace(\" \", \"\"))  # Exclude spaces\n",
        "\n",
        "        if gender_metadata[filename] == \"male\":\n",
        "            male_total_words += word_count\n",
        "            male_total_chars += char_count\n",
        "            male_word_errors += word_error_rate * word_count\n",
        "            male_char_errors += char_error_rate * char_count\n",
        "        elif gender_metadata[filename] == \"female\":\n",
        "            female_total_words += word_count\n",
        "            female_total_chars += char_count\n",
        "            female_word_errors += word_error_rate * word_count\n",
        "            female_char_errors += char_error_rate * char_count\n",
        "\n",
        "# Calculate overall WER and CER by gender\n",
        "male_wer = male_word_errors / male_total_words if male_total_words > 0 else 0\n",
        "female_wer = female_word_errors / female_total_words if female_total_words > 0 else 0\n",
        "\n",
        "male_cer = male_char_errors / male_total_chars if male_total_chars > 0 else 0\n",
        "female_cer = female_char_errors / female_total_chars if female_total_chars > 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(\"\\nOverall Results by Gender:\")\n",
        "print(f\"Male WER: {male_wer:.2%}\")\n",
        "print(f\"Male CER: {male_cer:.2%}\")\n",
        "print(f\"Female WER: {female_wer:.2%}\")\n",
        "print(f\"Female CER: {female_cer:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWWoeYc9Xvxc",
        "outputId": "409ab7fe-411d-459f-d670-4df9edbb7899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Results by Gender:\n",
            "Male WER: 37.14%\n",
            "Male CER: 25.98%\n",
            "Female WER: 56.52%\n",
            "Female CER: 40.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results: The WER and CER is much higher for female samples than male samples, indicating issues of intersectionality even though 66% of the samples are gathered on females... interesting (can talk about overrepresentation)"
      ],
      "metadata": {
        "id": "yznWvaaiX2c4"
      }
    }
  ]
}